"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ Hugging Face —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π.

–£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π JSON —Ñ–∞–π–ª–æ–≤ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
- category/video_id/snapshot_date/metadata.json

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–Ω–∞–ø—à–æ—Ç—ã –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.
"""

import json
import os
import re
import pickle
import httpx
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from transliterate import translit
from utils._static import CATEGORIES2TRANSLITERATE

try:
    from huggingface_hub import HfApi
    HF_HUB_AVAILABLE = True
except ImportError as e:
    HF_HUB_AVAILABLE = False
    print("Warning: huggingface_hub is not installed. Install it with: pip install huggingface_hub", e)

class HuggingFaceUploader:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ Hugging Face —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π.
    """
    
    def __init__(
        self,
        repo_id: str,
        token: Optional[str] = None,
        repo_type: str = "dataset",
        cache_dir: str = None,
        cat2ids_path: str = ".input/cat2ids.json"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            repo_id: ID —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ "username/repo_name"
            token: Hugging Face —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ HF_HUB_TOKEN env var)
            repo_type: –¢–∏–ø —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è ("dataset" –∏–ª–∏ "model")
        """
        if not HF_HUB_AVAILABLE:
            raise ImportError("huggingface_hub is required. Install it with: pip install huggingface_hub")

        self.cache_dir = cache_dir
        self.repo_id = repo_id
        self.repo_type = repo_type
        self.token = token
        self.api_url = f"https://huggingface.co/api/{repo_type}s/{repo_id}"
        self.cat2ids = json.load(open(cat2ids_path, "r"))
        self.headers = {"Authorization": f"Bearer {token}"}

        if os.path.exists(cache_dir):
            if os.path.exists(os.path.join(cache_dir, "repo_files.pkl")):
                self.current_repo_files = self.get_repo_files_cache()
            else:
                self.current_repo_files = self.get_current_repo_files()
        else:
            os.mkdir(cache_dir)
            self.current_repo_files = self.get_current_repo_files()
        
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω HuggingFaceUploader –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: {repo_id}")
        
        
    def get_files_from_commits(self, batch_size=100, timeout=120.0):
        files_state = 0
        cursor = 0
        with httpx.Client(timeout=timeout) as client:
            while True:
                url = f"{self.api_url}/commits/main?limit={batch_size}&cursor={cursor}"
                r = client.get(url, headers=self.headers)
                if r.status_code == 404:
                    raise ValueError("Repo not found or private (check token or repo_type)")
                r.raise_for_status()

                commits = r.json()
                if not commits:
                    break

                for commit in commits:
                    title = commit.get("title")
                    if "Batch upload" in title:
                        files_state += int(title.split(" ")[2])
                    else:
                        files_state += 1

                print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–º–º–∏—Ç–æ–≤: {cursor + len(commits)}")
                if len(commits) < batch_size:
                    break
                cursor += batch_size

        print(f"‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {files_state}")
        return files_state

    def get_repo_files_cache(self):
        files = pickle.load(open(f"{self.cache_dir}/repo_files.pkl", 'rb'))
        print(f"(get_repo_files_cache) –ü–æ–ª—É—á–µ–Ω–æ {files} —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫—ç—à–∞")
        return files

    def load_repo_files_cache(self, files):
        pickle.dump(files, open(f"{self.cache_dir}/repo_files.pkl", 'wb'))
        print(f"(load_repo_files_cache) –ó–∞–≥—Ä—É–∂–µ–Ω–æ {files} —Ñ–∞–π–ª–æ–≤ –≤ –∫—ç—à")

    def update_repo_files_cache(self, new_files_cnt):
        old_len = self.current_repo_files
        self.current_repo_files += new_files_cnt
        self.load_repo_files_cache(self.current_repo_files)

        print(f"(update_repo_files_cache) –ë—ã–ª–æ: {old_len} | –ü–æ–ª—É—á–µ–Ω–æ: {new_files_cnt} | –ö–æ–ª-–≤–æ –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {self.current_repo_files}")


    def get_current_repo_files(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.
        """

        cache_path = os.path.join(self.cache_dir, "repo_files.pkl")

        try:
            if not os.path.exists(cache_path):
                files = self.get_files_from_commits(timeout=300, batch_size=1000)

                with open(cache_path, "wb") as f:
                    pickle.dump(files, f)
            else:
                with open(cache_path, 'rb') as f:
                    files = pickle.load(f)

        except Exception as e:
            print(f"Get repo file error: {e}")
            files = 0

        # current_repo_files = []
        
        # for file in files:
        #     sp = file.split("/")
        #     category = sp[0]
        #     if category not in list(CATEGORIES2TRANSLITERATE.values()):
        #         continue
        #     if sp[1] not in self.cat2ids[category]:
        #         continue
        #     if "meta_" not in sp[2]:
        #         continue
        #     current_repo_files.append(file)
            
        return files
        
        
    def check_for_new_batches(self, success_batchs: List[int], yt_dlp_results_dir: str) -> None:
        
        new_batchs = []
        
        for batch_name in os.listdir(yt_dlp_results_dir):
            if batch_name.startswith("batch_") and batch_name.endswith(".json"):
                if batch_name not in success_batchs:
                    new_batchs.append(batch_name)
        
        return new_batchs
    
    
    def list_existing_snapshots(self, category: str, video_id: str) -> List[Tuple[str, bool]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –≤ HF.

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (snapshot_date, is_meta_snapshot), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ –¥–∞—Ç–µ
        """
        try:
            # –ü—É—Ç—å –ø–∞–ø–∫–∏ –≤–∏–¥–µ–æ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ (—É—á–∏—Ç—ã–≤–∞–µ–º —Ç—É –∂–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —á—Ç–æ –∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ)
            category_key = translit(category, "en", reversed=True).replace(' ', '_')
            video_path = f"{category_key}/{video_id}"

            try:
                files = self.api.list_repo_files(repo_id=self.repo_id, repo_type=self.repo_type)
            except Exception as e:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è {video_path}: {e}")
                return []

            snapshots: List[Tuple[str, bool]] = []
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ JSON —Ñ–∞–π–ª—ã –≤–Ω—É—Ç—Ä–∏ –ø–æ–¥–ø–∞–ø–æ–∫ –≤–∏–¥–µ–æ
            pattern = re.compile(rf"{re.escape(video_path)}/([^/]+)/.*\\.json")

            for file in files:
                match = pattern.match(file)
                if not match:
                    continue
                folder_name = match.group(1)
                is_meta = folder_name.startswith('meta_')
                snapshot_date = folder_name[5:] if is_meta else folder_name
                snapshot_tuple = (snapshot_date, is_meta)
                if snapshot_tuple not in snapshots:
                    snapshots.append(snapshot_tuple)

            return sorted(snapshots, key=lambda x: x[0])
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è {category}/{video_id}: {e}")
            return []
        

    def get_video_snapshot_status(self, category: str, video_id: str) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–∏–¥–µ–æ.

        Returns:
            {
              'meta_snapshot_date': Optional[str],
              'regular_snapshots': List[str],
            }
        """
        snapshots = self.list_existing_snapshots(category, video_id)
        meta_date: Optional[str] = None
        regular_dates: List[str] = []

        for snapshot_date, is_meta in snapshots:
            if is_meta:
                meta_date = snapshot_date
            else:
                regular_dates.append(snapshot_date)

        regular_dates = sorted(regular_dates)
        return {
            'meta_snapshot_date': meta_date,
            'regular_snapshots': regular_dates,
        }
        

    def determine_current_snapshot_number(self, video_list: List[Tuple[str, str]]) -> Tuple[int, int]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â–∏–π –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä —Å–Ω–∞–ø—à–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–∏—Å–∫–∞ –≤–∏–¥–µ–æ.

        video_list: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (category, video_url)

        Returns: (snapshot_number, checked_count)
        """
        if not video_list:
            print("  –°–ø–∏—Å–æ–∫ –≤–∏–¥–µ–æ –ø—É—Å—Ç, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞")
            return 1, 0

        max_completed: Optional[int] = None

        for idx, (category, video_url) in enumerate(video_list, start=1):
            try:
                vid = self.extract_video_id(video_url) or video_url.split('?v=')[-1]
                status = self.get_video_snapshot_status(category, vid)

                if status['meta_snapshot_date'] is None:
                    # –ù–µ—Ç meta snapshot ‚Äî —Å–ª–µ–¥—É—é—â–∏–π –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é: 1
                    return 1, idx - 1 if idx > 1 else 0
                else:
                    # –ï—Å—Ç—å meta snapshot ‚Äî —Å–Ω–∞–ø—à–æ—Ç 1 –∑–∞–≤–µ—Ä—à–µ–Ω; –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å–æ 2
                    completed_snapshot = 1 + len(status['regular_snapshots'])

                    if max_completed is None:
                        max_completed = completed_snapshot
                    else:
                        max_completed = max(max_completed, completed_snapshot)

                    if completed_snapshot < max_completed:
                        next_snapshot = completed_snapshot + 1
                        print(f"  {vid}: –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–æ {completed_snapshot}, –º–∞–∫—Å–∏–º—É–º {max_completed}, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º {next_snapshot}")
                        return next_snapshot, idx - 1

            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤–∏–¥–µ–æ {idx}: {e}")
                return 1, idx

        if max_completed is None:
            return 1, 0
        return max_completed + 1, 0
    
    
    def extract_video_id(self, video_url: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç video ID –∏–∑ YouTube URL.
        
        Args:
            video_url: URL –≤–∏–¥–µ–æ YouTube
            
        Returns:
            video_id –∏–ª–∏ None
        """
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'youtube\.com\/watch\?v=([0-9A-Za-z_-]{11})',
            r'youtu\.be\/([0-9A-Za-z_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, video_url)
            if match:
                return match.group(1)
        
        return None

    
    def upload_metadata(
        self, 
        metadata: Dict[str, Any], 
        category: str, 
        video_id: str, 
        folder_name: Optional[str] = None, 
    ) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –≤ HF.
        
        Args:
            metadata: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ
            category: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ä—É—Å—Å–∫–æ–µ)
            video_id: ID –≤–∏–¥–µ–æ
            meta_folder_name: –ò–º—è –ø–∞–ø–∫–∏ —Å–Ω–∞–ø—à–æ—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è)
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        hf_path = f"{category}/{video_id}/{folder_name}"
        
        try:
            # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON
            json_content = json.dumps(metadata, ensure_ascii=False, indent=2, default=str)
            json_bytes = json_content.encode('utf-8')
            
            try:
                self.api.upload_file(
                    path_or_fileobj=json_bytes,
                    path_in_repo=hf_path,
                    repo_id=self.repo_id,
                    repo_type=self.repo_type,
                    token=self.token
                )
            except Exception as e:
                print(f"Error uploading metadata to {hf_path}: {e}")
                return False
            
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {hf_path}")
            return True
            
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {hf_path}: {e}")
            return False
    
    def upload_metadata_batch(
        self,
        files_to_upload: List[Tuple[Dict[str, Any], str, str, str]],
        folder_name: str
    ) -> Tuple[int, int]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ–¥–Ω–∏–º –∫–æ–º–º–∏—Ç–æ–º.
        
        Args:
            files_to_upload: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (metadata, category, video_id, hf_path)
            folder_name: –ò–º—è –ø–∞–ø–∫–∏ —Å–Ω–∞–ø—à–æ—Ç–∞
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ, –≤—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫)
        """
        if not files_to_upload:
            return 0, 0
        
        import tempfile
        import shutil
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        temp_dir = tempfile.mkdtemp()
        uploaded_count = 0
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –∏ —Ñ–∞–π–ª–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            for metadata, category, video_id, hf_path in files_to_upload:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∫ –ø—É—Ç–∏ (–µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç)
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å: category/video_id/folder_name/metadata.json
                if not hf_path.endswith('.json'):
                    # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ .json, –¥–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º video_id –∫–∞–∫ –∏–º—è —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                    hf_file_path = f"{hf_path}/{video_id}.json"
                else:
                    hf_file_path = hf_path
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                local_path = os.path.join(temp_dir, hf_file_path)
                os.makedirs(os.path.dirname(local_path), exist_ok=True)
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º JSON —Ñ–∞–π–ª
                json_content = json.dumps(metadata, ensure_ascii=False, indent=2, default=str)
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(json_content)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—é –ø–∞–ø–∫—É –æ–¥–Ω–∏–º –∫–æ–º–º–∏—Ç–æ–º
            try:
                from huggingface_hub import upload_folder
                upload_folder(
                    folder_path=temp_dir,
                    repo_id=self.repo_id,
                    repo_type=self.repo_type,
                    token=self.token,
                    commit_message=f"Batch upload: {len(files_to_upload)} metadata files"
                )
                uploaded_count = len(files_to_upload)
                print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤—Å–µ–≥–æ: {uploaded_count}")
            except Exception as e:
                error_str = str(e)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π rate limit
                if "429" in error_str or "rate limit" in error_str.lower() or "Too Many Requests" in error_str:
                    print(f"Rate limit reached during batch upload. Error: {e}")
                    print("Waiting 60 seconds before retry...")
                    time.sleep(60)
                    # –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                    try:
                        from huggingface_hub import upload_folder
                        upload_folder(
                            folder_path=temp_dir,
                            repo_id=self.repo_id,
                            repo_type=self.repo_type,
                            token=self.token,
                            commit_message=f"Batch upload: {len(files_to_upload)} metadata files (retry)"
                        )
                        uploaded_count = len(files_to_upload)
                        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤—Å–µ–≥–æ: {uploaded_count}")
                    except Exception as e2:
                        print(f"Retry failed: {e2}")
                        print("Please wait for rate limit to reset (about 1 hour)")
                        raise
                else:
                    print(f"Error uploading batch: {e}")
                    # –ï—Å–ª–∏ batch upload –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
                    # (fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
                    print("Falling back to individual uploads...")
                    for metadata, category, video_id, hf_path in files_to_upload:
                        if self.upload_metadata(metadata, category, video_id, folder_name):
                            uploaded_count += 1
                        time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        return uploaded_count, len(files_to_upload)
    
    
    def upload_from_file(self, json_file: str, category: str, video_url: str,
                        snapshot_date: Optional[str] = None, overwrite: bool = False) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ HF.
        
        Args:
            json_file: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            category: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ä—É—Å—Å–∫–æ–µ)
            video_url: URL –≤–∏–¥–µ–æ (–¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è video_id)
            snapshot_date: –î–∞—Ç–∞ —Å–Ω–∞–ø—à–æ—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è)
            overwrite: –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –µ—Å–ª–∏ True
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            return self.upload_metadata(metadata, category, video_url, snapshot_date, overwrite)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {json_file}: {e}")
            return False


# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å urls.json (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–Ω–µ –∫–ª–∞—Å—Å–∞)
def load_urls_data(urls_file: str) -> Dict[str, Any]:
    with open(urls_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_flat_video_list(urls_data: Dict[str, Any]) -> List[Tuple[str, str]]:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É urls.json –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ (category, video_url)
    —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞ –æ–±—Ö–æ–¥–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
    """
    video_list: List[Tuple[str, str]] = []
    for category, intervals in urls_data.items():
        if not isinstance(intervals, dict):
            continue
        for _interval, videos in intervals.items():
            if not isinstance(videos, dict):
                continue
            for video_url, video_data in videos.items():
                if isinstance(video_data, dict):
                    video_list.append((category, video_url))
    return video_list
